{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo\\.conda\\envs\\mol\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# New task: how much should we train the model? how many iterations? effect on clusters?\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from hflayers import Hopfield\n",
    "\n",
    "from CSIP.utils import unload_data\n",
    "from CSIP.dataset import GraphDataset, GraphLoader\n",
    "from CSIP.model.GIN import GNN\n",
    "from CSIP.training import train, evaluate\n",
    "\n",
    "class parameters: pass\n",
    "args = parameters()\n",
    "\n",
    "args.lr = 0.005 ###\n",
    "\n",
    "args.eval_batch = 1\n",
    "args.train_size = 0.80\n",
    "args.batch_size = 2500 #2500 # 15% verified. \n",
    "losses = [] # 20%\n",
    "args.device = \"cpu\"\n",
    "args.loss = 'standard' ## verified\n",
    "args.save_dir = 'C:/Users/leo/Desktop/realresearch/output'\n",
    "args.load_dir = 'D:/Cell painting/20000' #'D:/Cell painting/output_oneplate'  #'D:/Cell painting' # \n",
    "args.input_dim = 256\n",
    "args.hidden_dim = 64 ## verified\n",
    "args.num_mlps = 2\n",
    "args.num_classes = 8\n",
    "args.num_layers = 3 ## verified\n",
    "args.pad = 80 ## verified\n",
    "args.graph_dim = 64\n",
    "args.scale_hopfield = None\n",
    "args.precision = 'amp'\n",
    "args.use_tensorboard = False\n",
    "args.use_wandb = True\n",
    "args.debug = True\n",
    "args.dropout = 0.3\n",
    "args.inv_tau = True # turn on learnable inv tau\n",
    "args.eval_step = 3\n",
    "\n",
    "# use lstm?\n",
    "CSIP_img = GNN(args.input_dim, args.num_classes, num_layers = args.num_layers,\n",
    "            hidden_dim = args.hidden_dim, num_mlps = args.num_mlps, pad = args.pad, graph_dim=args.graph_dim, \n",
    "            learnable_inv_tau = args.inv_tau, init_inv_tau = 2.71828, use_lstm = False)\n",
    "CSIP_img.train()\n",
    "CSIP_img = CSIP_img.to(args.device)\n",
    "\n",
    "# hyperparameter tweaking required.\n",
    "CSIP_mol = GNN(45, args.num_classes, num_layers = args.num_layers,\n",
    "            hidden_dim = args.hidden_dim, num_mlps = args.num_mlps, pad = 45, graph_dim=args.graph_dim, \n",
    "            use_lstm = False)\n",
    "CSIP_mol.train()\n",
    "CSIP_mol = CSIP_mol.to(args.device)\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "def get_cosine_scheduler(\n",
    "        optimizer: Optimizer, warmup: int = 5, \n",
    "        num_training_steps: int = 20, num_cycles: float = 0.5, last_epoch: int = -1 \n",
    "):\n",
    "    '''\n",
    "    Args:\n",
    "        optimizer (:class:`~torch.optim.Optimizer`):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        warmup (:obj:`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (:obj:`int`):\n",
    "            The total number of training steps.\n",
    "        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n",
    "            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
    "            following a half-cosine).\n",
    "        last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "    '''\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup:\n",
    "\n",
    "            # return float(step) / float(max(1, warmup)) # more conservative\n",
    "            return 1.0 # aggressive\n",
    "        \n",
    "        progress = float(step - warmup) / float(max(1, num_training_steps - warmup))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "# if using amp precision\n",
    "if args.precision == 'amp':\n",
    "    scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class baseline_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, init_inv_tau = 14.3):\n",
    "        super(baseline_MLP, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim * num_layers)\n",
    "        self.logit_inv_tau = nn.Parameter(torch.ones([]) * np.log(init_inv_tau))\n",
    "        self.logit_inv_tau.requires_grad = True\n",
    "\n",
    "    def forward(self, graphs, features):\n",
    "        features = features.mean(dim = 1) \n",
    "        features = self.linear(features)\n",
    "        return features\n",
    "    \n",
    "baseline_img = baseline_MLP(args.input_dim, args.hidden_dim, args.num_layers)\n",
    "baseline_img.train()\n",
    "baseline_img = baseline_img.to(args.device)\n",
    "\n",
    "baseline_mol = GNN(45, args.num_classes, num_layers = args.num_layers,\n",
    "            hidden_dim = args.hidden_dim, num_mlps = args.num_mlps, pad = 45, graph_dim=args.graph_dim, \n",
    "            use_lstm = False)\n",
    "baseline_mol.train()\n",
    "basline_mol = baseline_mol.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graphs...\n",
      "Loading nodes...\n",
      "Loading ground truth label...\n"
     ]
    }
   ],
   "source": [
    "# graph, nodes, all_labels = unload_data(args.load_dir)\n",
    "# wells, perts, labels, index = all_labels\n",
    "# smiles = torch.load(os.path.join(args.load_dir, \"smiles.pth\"))\n",
    "\n",
    "graph, nodes, labels = unload_data(args.load_dir, load_label=1)\n",
    "\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.sort(labels)\n",
    "class_begins, class_size = [0], [0]\n",
    "num_classes = 1\n",
    "\n",
    "for i in range(1, len(labels)):\n",
    "\n",
    "    if labels[i] != labels[i - 1]:\n",
    "        class_begins.append(i)\n",
    "        class_size.append(0)\n",
    "        num_classes += 1\n",
    "    class_size[len(class_size) - 1] += 1\n",
    "\n",
    "class_begins.append(len(labels))\n",
    "classes = np.arange(num_classes)\n",
    "np.random.shuffle(classes)\n",
    "\n",
    "# zero-shot\n",
    "# train_classes = round(num_classes * args.train_size)    \n",
    "# train_idxs = np.array([j for i in classes[:train_classes] for j in range(class_begins[i], class_begins[i + 1])] )\n",
    "# test_idxs = np.array([j for i in classes[train_classes:] for j in range(class_begins[i], class_begins[i + 1])] )\n",
    "\n",
    "# random assignment\n",
    "# train_sample = int(args.train_size * len(labels))\n",
    "# idxs = np.random.permutation(len(labels))\n",
    "# train_idxs = idxs[:train_sample]\n",
    "# test_idxs = idxs[train_sample:]\n",
    "\n",
    "train_idxs, test_idxs = [], []\n",
    "for i in range(len(class_size)):\n",
    "\n",
    "    if class_size[i] > 1000: continue ########## skip DMSO, needed?\n",
    "\n",
    "    if class_size[i] < 12: \n",
    "        train_idxs.extend([i for i in range(class_begins[i], class_begins[i + 1])])\n",
    "        continue\n",
    "    n_samples = int(args.train_size * class_size[i])\n",
    "    idxs = np.random.permutation(range(class_begins[i], class_begins[i + 1]))\n",
    "    train_idxs.extend(idxs[:n_samples])\n",
    "    test_idxs.extend(idxs[n_samples:])\n",
    "\n",
    "# text idxs should not contain any compounds already trained on the model,\n",
    "# since we are evaluating zero-shot classification accuracy\n",
    "np.random.shuffle(train_idxs)\n",
    "np.random.shuffle(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SMILES...: 100%|██████████| 20000/20000 [01:17<00:00, 257.09it/s] \n",
      "Processing SMILES...: 100%|██████████| 20000/20000 [01:49<00:00, 182.30it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = GraphDataset(features = nodes, graphs = graph, labels = labels, \n",
    "                             idxs = train_idxs, \n",
    "                             pad = args.pad, size = args.input_dim, \n",
    "                             drop_edge=args.dropout, norm = False) \n",
    "stats, stats_label = train_dataset.stats, train_dataset.stats_label\n",
    "train_loader = GraphLoader(train_dataset, batch_size = args.batch_size)\n",
    "test_dataset = GraphDataset(features = nodes, graphs = graph, labels = labels, \n",
    "                             idxs = test_idxs, \n",
    "                             pad = args.pad, size = args.input_dim, \n",
    "                             norm = False, drop_edge=0.0,\n",
    "                             stats = stats, stats_label = stats_label)\n",
    "test_loader = GraphLoader(test_dataset, batch_size = args.batch_size) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_per_epoch = max(1, int(len(train_loader) / args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_img, model_mol, iters = 50, save = False, ckpt = None):\n",
    "\n",
    "    gc.collect()\n",
    "    model_img.train()\n",
    "    model_mol.train()\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    args.use_wandb = False\n",
    "    start_iter = 0\n",
    "    args.iters = iters\n",
    "\n",
    "    optimizer_img = optim.AdamW(model_img.parameters(), args.lr) \n",
    "    optimizer_mol = optim.AdamW(model_mol.parameters(), args.lr)\n",
    "    scheduler_img = get_cosine_scheduler(optimizer_img)\n",
    "    scheduler_mol = get_cosine_scheduler(optimizer_mol)\n",
    "\n",
    "    if ckpt is not None:\n",
    "\n",
    "        ckpt = torch.load(ckpt)\n",
    "        start_iter = ckpt['iter']\n",
    "\n",
    "        model_img.load_state_dict(ckpt['model_img_state'])\n",
    "        optimizer_img.load_state_dict(ckpt['optimizer_img'])\n",
    "        if scheduler_img is not None and \"scheduler_img\" in ckpt:\n",
    "            scheduler_img.load_state_dict(ckpt['scheduler_img'])\n",
    "\n",
    "        model_mol.load_state_dict(ckpt['model_mol_state'])\n",
    "        optimizer_mol.load_state_dict(ckpt['optimizer_mol'])\n",
    "        if scheduler_mol is not None and \"scheduler_mol\" in ckpt:\n",
    "            scheduler_mol.load_state_dict(ckpt['scheduler_mol'])\n",
    "\n",
    "        logging.info(\"All keys are matched successfully.\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"Checkpoint not available. Using random initialization instead.\")\n",
    "\n",
    "    if args.use_tensorboard == True:\n",
    "        Writer = SummaryWriter(args.save_dir)\n",
    "\n",
    "    if args.use_wandb == True:\n",
    "        logging.debug(\"Starting wandb.\")\n",
    "        wandb.init(\n",
    "            project = 'img2mol'\n",
    "        )\n",
    "        if args.debug:\n",
    "            wandb.watch(CSIP_img, log = 'all')\n",
    "            wandb.watch(model_mol, log = 'all')\n",
    "\n",
    "        logging.debug(\"Finish loading wandb.\")\n",
    "\n",
    "    iters_per_epoch = int(len(train_loader) / args.batch_size)\n",
    "    scheduler_img.step()\n",
    "    scheduler_mol.step()\n",
    "    for i in tqdm(range(start_iter, iters * args.batch_per_epoch)):\n",
    "\n",
    "        if (i + 1) % (args.eval_step) == 0: #((i + 1) % iters_per_epoch == 0) & (i >= 1):\n",
    "            scheduler_img.step()\n",
    "            scheduler_mol.step()\n",
    "            \n",
    "            # model_img.eval() # hypothesis: different number of cells - profiles not on the same magnitude - must do average based on non-emty cells\n",
    "            # model_mol.eval()\n",
    "            # eval_acc, train_acc = 0, 0\n",
    "\n",
    "            # for j in range(args.eval_batch):\n",
    "\n",
    "            #     features, graphs, labels, number_of_nodes = next(test_loader)\n",
    "            #     features = features.to(args.device)\n",
    "            #     labels = labels.to(args.device)\n",
    "            #     with torch.no_grad():\n",
    "            #         preds = model(graphs, number_of_nodes, features)\n",
    "            #     preds = np.argmax(preds, axis = 1)\n",
    "            #     labels = np.argmax(labels, axis = 1)\n",
    "            #     eval_acc += sum(preds == labels)\n",
    "\n",
    "            # eval_acc = eval_acc.item() / (args.eval_batch * args.batch_size)\n",
    "\n",
    "            # for j in range(args.eval_batch):\n",
    "\n",
    "            #     features, graphs, labels, number_of_nodes = next(train_loader)\n",
    "            #     features = features.to(args.device)\n",
    "            #     labels = labels.to(args.device)\n",
    "\n",
    "            #     with torch.no_grad():\n",
    "            #         preds = model(graphs, number_of_nodes, features)\n",
    "            #     preds = np.argmax(preds, axis = 1)\n",
    "            #     labels = np.argmax(labels, axis = 1)\n",
    "            #     train_acc += sum(preds == labels)\n",
    "            \n",
    "            # train_acc = train_acc.item() / (args.eval_batch * args.batch_size)\n",
    "            # logging.info(f'eval on iters {i + 1}, eval_acc = {eval_acc:3f}, train_acc = {train_acc:3f}')\n",
    "            \n",
    "            # model_img.train()\n",
    "            # model_mol.train()\n",
    "            \n",
    "            logging.info(f\"evaluating...\")\n",
    "\n",
    "            logging.info(f\"--------------- Zero-shot Eval ---------------\")\n",
    "            for _ in range(args.eval_batch):\n",
    "\n",
    "                features, graphs, (labels_graphs, labels_features), _ = next(test_loader)\n",
    "                features = features.to(args.device)\n",
    "\n",
    "                batch = ((graphs, features), (labels_graphs, labels_features))\n",
    "                    \n",
    "                evaluate(model_img, model_mol, batch, args, \n",
    "                    n_iter = i, tb_writer=None)#Writer)\n",
    "                \n",
    "            logging.info(f\"--------------- Training data eval ---------------\")\n",
    "\n",
    "            for _ in range(args.eval_batch):\n",
    "\n",
    "                features, graphs, (labels_graphs, labels_features), _ = next(train_loader)\n",
    "                features = features.to(args.device)\n",
    "\n",
    "                batch = ((graphs, features), (labels_graphs, labels_features))\n",
    "                    \n",
    "                evaluate(model_img, model_mol, batch, args, \n",
    "                    n_iter = i, zero_shot = False, tb_writer=None)#Writer)\n",
    "\n",
    "            logging.info(f\"--------------- Eval complete ---------------\")\n",
    "\n",
    "\n",
    "        if ((i + 1) % (args.batch_per_epoch) == 0) & save:\n",
    "            logging.info(f\"iters {i + 1}, saving...\")\n",
    "            state = {\n",
    "                \"iter\": i + 1,\n",
    "                \"model_img_state\": model_img.state_dict(),\n",
    "                \"model_mol_state\": model_mol.state_dict(),\n",
    "                \"optimizer_img\": optimizer_img.state_dict(),\n",
    "                \"optimizer_mol\": optimizer_mol.state_dict(),\n",
    "                \"scheduler_img\": scheduler_img.state_dict(),\n",
    "                \"scheduler_mol\": scheduler_mol.state_dict()\n",
    "            }\n",
    "            torch.save(state, os.path.join(args.save_dir, f'{i+1}.pth'))\n",
    "        \n",
    "        features, graphs, (labels_graphs, labels_features), _ = next(train_loader)\n",
    "        features = features.to(args.device)\n",
    "\n",
    "        batch = ((graphs, features), (labels_graphs, labels_features))\n",
    "        \n",
    "        train(model_img, model_mol, optimizer_img, optimizer_mol, \n",
    "            scaler, batch, args, \n",
    "            n_iter = i, tb_writer=None)#Writer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(CSIP_img, CSIP_mol, iters = 20, ckpt = 'C:/Users/leo/Desktop/realresearch/output/2500-20000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checkpoint not available. Using random initialization instead.\n",
      "c:\\Users\\leo\\.conda\\envs\\mol\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]c:\\Users\\leo\\.conda\\envs\\mol\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "INFO:root:Train Iters: 0 [0/40 (0%)]\tLoss: 2.941244\tData (t) 0.000\tBatch (t) 193.081\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.370\n",
      "  2%|▎         | 1/40 [03:14<2:06:08, 194.07s/it]INFO:root:Train Iters: 1 [1/40 (2%)]\tLoss: 2.946627\tData (t) 0.000\tBatch (t) 150.834\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.440\n",
      "  5%|▌         | 2/40 [05:46<1:47:21, 169.51s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 2 [2/40 (5%)]\tLoss: 2.930495\tTop1 Acc: 0.017200\tTop5 Acc: 0.021200\tTop10 Acc: 0.026000\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 2 [2/40 (5%)]\tLoss: 2.932930\tTop1 Acc: 0.132000\tTop5 Acc: 0.146400\tTop10 Acc: 0.147200\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 2 [2/40 (5%)]\tLoss: 2.931663\tData (t) 0.000\tBatch (t) 151.616\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.510\n",
      "  8%|▊         | 3/40 [09:11<1:54:36, 185.85s/it]INFO:root:iters 4, saving...\n",
      "INFO:root:Train Iters: 3 [3/40 (8%)]\tLoss: 2.924322\tData (t) 0.000\tBatch (t) 144.866\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.581\n",
      " 10%|█         | 4/40 [11:37<1:41:54, 169.86s/it]INFO:root:Train Iters: 4 [4/40 (10%)]\tLoss: 2.919515\tData (t) 0.000\tBatch (t) 149.743\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.652\n",
      " 12%|█▎        | 5/40 [14:07<1:34:56, 162.75s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 5 [5/40 (12%)]\tLoss: 2.914498\tTop1 Acc: 0.023600\tTop5 Acc: 0.024400\tTop10 Acc: 0.042800\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 5 [5/40 (12%)]\tLoss: 2.914964\tTop1 Acc: 0.028000\tTop5 Acc: 0.031600\tTop10 Acc: 0.038000\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 5 [5/40 (12%)]\tLoss: 2.914911\tData (t) 0.000\tBatch (t) 154.802\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.724\n",
      " 15%|█▌        | 6/40 [17:33<1:40:32, 177.42s/it]INFO:root:Train Iters: 6 [6/40 (15%)]\tLoss: 2.909092\tData (t) 0.000\tBatch (t) 165.410\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.796\n",
      " 18%|█▊        | 7/40 [20:18<1:35:27, 173.55s/it]INFO:root:iters 8, saving...\n",
      "INFO:root:Train Iters: 7 [7/40 (18%)]\tLoss: 2.903339\tData (t) 0.000\tBatch (t) 162.628\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.868\n",
      " 20%|██        | 8/40 [23:01<1:30:44, 170.14s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 8 [8/40 (20%)]\tLoss: 2.898732\tTop1 Acc: 0.024000\tTop5 Acc: 0.024000\tTop10 Acc: 0.027200\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 8 [8/40 (20%)]\tLoss: 2.898661\tTop1 Acc: 0.025200\tTop5 Acc: 0.031600\tTop10 Acc: 0.035600\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 8 [8/40 (20%)]\tLoss: 2.899042\tData (t) 0.000\tBatch (t) 168.814\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 14.940\n",
      " 22%|██▎       | 9/40 [26:51<1:37:37, 188.95s/it]INFO:root:Train Iters: 9 [9/40 (22%)]\tLoss: 2.893277\tData (t) 0.000\tBatch (t) 143.845\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 15.013\n",
      " 25%|██▌       | 10/40 [29:16<1:27:35, 175.19s/it]INFO:root:Train Iters: 10 [10/40 (25%)]\tLoss: 2.887906\tData (t) 0.000\tBatch (t) 146.455\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 15.086\n",
      " 28%|██▊       | 11/40 [31:43<1:20:31, 166.60s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 11 [11/40 (28%)]\tLoss: 2.883205\tTop1 Acc: 0.009600\tTop5 Acc: 0.012000\tTop10 Acc: 0.012000\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 11 [11/40 (28%)]\tLoss: 2.882688\tTop1 Acc: 0.032400\tTop5 Acc: 0.038000\tTop10 Acc: 0.039600\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:iters 12, saving...\n",
      "INFO:root:Train Iters: 11 [11/40 (28%)]\tLoss: 2.882988\tData (t) 0.000\tBatch (t) 172.305\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 15.160\n",
      " 30%|███       | 12/40 [35:28<1:26:00, 184.32s/it]INFO:root:Train Iters: 12 [12/40 (30%)]\tLoss: 2.877722\tData (t) 0.000\tBatch (t) 195.949\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 15.234\n",
      " 32%|███▎      | 13/40 [38:44<1:24:33, 187.90s/it]INFO:root:Train Iters: 13 [13/40 (32%)]\tLoss: 2.872555\tData (t) 0.000\tBatch (t) 225.808\tLR_imgs: 0.005000 \tLR_mol: 0.005000 \tinv_tau 15.308\n",
      " 35%|███▌      | 14/40 [42:30<1:26:27, 199.51s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 14 [14/40 (35%)]\tLoss: 2.867816\tTop1 Acc: 0.009600\tTop5 Acc: 0.009600\tTop10 Acc: 0.023200\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 14 [14/40 (35%)]\tLoss: 2.867242\tTop1 Acc: 0.015600\tTop5 Acc: 0.023600\tTop10 Acc: 0.026400\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 14 [14/40 (35%)]\tLoss: 2.867080\tData (t) 0.000\tBatch (t) 275.219\tLR_imgs: 0.004945 \tLR_mol: 0.004945 \tinv_tau 15.382\n",
      " 38%|███▊      | 15/40 [48:41<1:44:39, 251.18s/it]INFO:root:iters 16, saving...\n",
      "INFO:root:Train Iters: 15 [15/40 (38%)]\tLoss: 2.861548\tData (t) 0.000\tBatch (t) 251.174\tLR_imgs: 0.004945 \tLR_mol: 0.004945 \tinv_tau 15.456\n",
      " 40%|████      | 16/40 [52:53<1:40:33, 251.40s/it]INFO:root:Train Iters: 16 [16/40 (40%)]\tLoss: 2.857127\tData (t) 0.000\tBatch (t) 251.268\tLR_imgs: 0.004945 \tLR_mol: 0.004945 \tinv_tau 15.531\n",
      " 42%|████▎     | 17/40 [57:05<1:36:26, 251.61s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 17 [17/40 (42%)]\tLoss: 2.853072\tTop1 Acc: 0.008400\tTop5 Acc: 0.013600\tTop10 Acc: 0.026400\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 17 [17/40 (42%)]\tLoss: 2.852462\tTop1 Acc: 0.003200\tTop5 Acc: 0.008800\tTop10 Acc: 0.010800\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 17 [17/40 (42%)]\tLoss: 2.852227\tData (t) 0.000\tBatch (t) 252.779\tLR_imgs: 0.004784 \tLR_mol: 0.004784 \tinv_tau 15.603\n",
      " 45%|████▌     | 18/40 [1:02:56<1:43:09, 281.35s/it]INFO:root:Train Iters: 18 [18/40 (45%)]\tLoss: 2.847065\tData (t) 0.000\tBatch (t) 251.475\tLR_imgs: 0.004784 \tLR_mol: 0.004784 \tinv_tau 15.676\n",
      " 48%|████▊     | 19/40 [1:07:07<1:35:21, 272.47s/it]INFO:root:iters 20, saving...\n",
      "INFO:root:Train Iters: 19 [19/40 (48%)]\tLoss: 2.842173\tData (t) 0.000\tBatch (t) 243.162\tLR_imgs: 0.004784 \tLR_mol: 0.004784 \tinv_tau 15.749\n",
      " 50%|█████     | 20/40 [1:11:11<1:27:55, 263.77s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 20 [20/40 (50%)]\tLoss: 2.838378\tTop1 Acc: 0.008400\tTop5 Acc: 0.010000\tTop10 Acc: 0.016800\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 20 [20/40 (50%)]\tLoss: 2.837782\tTop1 Acc: 0.004800\tTop5 Acc: 0.009600\tTop10 Acc: 0.012800\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 20 [20/40 (50%)]\tLoss: 2.837284\tData (t) 0.000\tBatch (t) 239.663\tLR_imgs: 0.004523 \tLR_mol: 0.004523 \tinv_tau 15.818\n",
      " 52%|█████▎    | 21/40 [1:16:39<1:29:37, 283.04s/it]INFO:root:Train Iters: 21 [21/40 (52%)]\tLoss: 2.832957\tData (t) 0.000\tBatch (t) 248.936\tLR_imgs: 0.004523 \tLR_mol: 0.004523 \tinv_tau 15.888\n",
      " 55%|█████▌    | 22/40 [1:20:48<1:21:51, 272.84s/it]INFO:root:Train Iters: 22 [22/40 (55%)]\tLoss: 2.828213\tData (t) 0.000\tBatch (t) 260.678\tLR_imgs: 0.004523 \tLR_mol: 0.004523 \tinv_tau 15.958\n",
      " 57%|█████▊    | 23/40 [1:25:09<1:16:18, 269.35s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 23 [23/40 (58%)]\tLoss: 2.825137\tTop1 Acc: 0.012800\tTop5 Acc: 0.013600\tTop10 Acc: 0.032800\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 23 [23/40 (58%)]\tLoss: 2.824352\tTop1 Acc: 0.004400\tTop5 Acc: 0.007200\tTop10 Acc: 0.011600\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:iters 24, saving...\n",
      "INFO:root:Train Iters: 23 [23/40 (58%)]\tLoss: 2.823860\tData (t) 0.000\tBatch (t) 237.739\tLR_imgs: 0.004173 \tLR_mol: 0.004173 \tinv_tau 16.023\n",
      " 60%|██████    | 24/40 [1:30:38<1:16:37, 287.34s/it]INFO:root:Train Iters: 24 [24/40 (60%)]\tLoss: 2.820260\tData (t) 0.000\tBatch (t) 227.493\tLR_imgs: 0.004173 \tLR_mol: 0.004173 \tinv_tau 16.088\n",
      " 62%|██████▎   | 25/40 [1:34:26<1:07:21, 269.45s/it]INFO:root:Train Iters: 25 [25/40 (62%)]\tLoss: 2.816169\tData (t) 0.000\tBatch (t) 240.094\tLR_imgs: 0.004173 \tLR_mol: 0.004173 \tinv_tau 16.153\n",
      " 65%|██████▌   | 26/40 [1:38:26<1:00:50, 260.72s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 26 [26/40 (65%)]\tLoss: 2.812911\tTop1 Acc: 0.009200\tTop5 Acc: 0.009200\tTop10 Acc: 0.026400\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 26 [26/40 (65%)]\tLoss: 2.811411\tTop1 Acc: 0.004000\tTop5 Acc: 0.008800\tTop10 Acc: 0.013200\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 26 [26/40 (65%)]\tLoss: 2.810882\tData (t) 0.000\tBatch (t) 233.467\tLR_imgs: 0.003750 \tLR_mol: 0.003750 \tinv_tau 16.212\n",
      " 68%|██████▊   | 27/40 [1:43:51<1:00:37, 279.82s/it]INFO:root:iters 28, saving...\n",
      "INFO:root:Train Iters: 27 [27/40 (68%)]\tLoss: 2.807418\tData (t) 0.000\tBatch (t) 220.680\tLR_imgs: 0.003750 \tLR_mol: 0.003750 \tinv_tau 16.272\n",
      " 70%|███████   | 28/40 [1:47:32<52:26, 262.24s/it]  INFO:root:Train Iters: 28 [28/40 (70%)]\tLoss: 2.804448\tData (t) 0.000\tBatch (t) 233.805\tLR_imgs: 0.003750 \tLR_mol: 0.003750 \tinv_tau 16.331\n",
      " 72%|███████▎  | 29/40 [1:51:26<46:32, 253.84s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 29 [29/40 (72%)]\tLoss: 2.801618\tTop1 Acc: 0.017200\tTop5 Acc: 0.019600\tTop10 Acc: 0.026400\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 29 [29/40 (72%)]\tLoss: 2.800540\tTop1 Acc: 0.014000\tTop5 Acc: 0.020800\tTop10 Acc: 0.027200\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 29 [29/40 (72%)]\tLoss: 2.800510\tData (t) 0.000\tBatch (t) 418.020\tLR_imgs: 0.003273 \tLR_mol: 0.003273 \tinv_tau 16.383\n",
      " 75%|███████▌  | 30/40 [1:59:56<55:05, 330.54s/it]INFO:root:Train Iters: 30 [30/40 (75%)]\tLoss: 2.797159\tData (t) 0.000\tBatch (t) 432.348\tLR_imgs: 0.003273 \tLR_mol: 0.003273 \tinv_tau 16.435\n",
      " 78%|███████▊  | 31/40 [2:07:09<54:10, 361.21s/it]INFO:root:iters 32, saving...\n",
      "INFO:root:Train Iters: 31 [31/40 (78%)]\tLoss: 2.794299\tData (t) 0.000\tBatch (t) 439.428\tLR_imgs: 0.003273 \tLR_mol: 0.003273 \tinv_tau 16.488\n",
      " 80%|████████  | 32/40 [2:14:29<51:18, 384.85s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 32 [32/40 (80%)]\tLoss: 2.792423\tTop1 Acc: 0.015600\tTop5 Acc: 0.018800\tTop10 Acc: 0.022000\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 32 [32/40 (80%)]\tLoss: 2.790642\tTop1 Acc: 0.021200\tTop5 Acc: 0.028400\tTop10 Acc: 0.030800\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 32 [32/40 (80%)]\tLoss: 2.790904\tData (t) 0.000\tBatch (t) 433.176\tLR_imgs: 0.002761 \tLR_mol: 0.002761 \tinv_tau 16.532\n",
      " 82%|████████▎ | 33/40 [2:24:59<53:29, 458.56s/it]INFO:root:Train Iters: 33 [33/40 (82%)]\tLoss: 2.788134\tData (t) 0.000\tBatch (t) 434.155\tLR_imgs: 0.002761 \tLR_mol: 0.002761 \tinv_tau 16.576\n",
      " 85%|████████▌ | 34/40 [2:32:14<45:08, 451.36s/it]INFO:root:Train Iters: 34 [34/40 (85%)]\tLoss: 2.785410\tData (t) 0.000\tBatch (t) 439.580\tLR_imgs: 0.002761 \tLR_mol: 0.002761 \tinv_tau 16.621\n",
      " 88%|████████▊ | 35/40 [2:39:34<37:19, 447.96s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 35 [35/40 (88%)]\tLoss: 2.784042\tTop1 Acc: 0.016800\tTop5 Acc: 0.016800\tTop10 Acc: 0.026800\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 35 [35/40 (88%)]\tLoss: 2.783088\tTop1 Acc: 0.010400\tTop5 Acc: 0.015200\tTop10 Acc: 0.016400\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:iters 36, saving...\n",
      "INFO:root:Train Iters: 35 [35/40 (88%)]\tLoss: 2.782116\tData (t) 0.000\tBatch (t) 429.953\tLR_imgs: 0.002239 \tLR_mol: 0.002239 \tinv_tau 16.657\n",
      " 90%|█████████ | 36/40 [2:50:05<33:31, 502.95s/it]INFO:root:Train Iters: 36 [36/40 (90%)]\tLoss: 2.781037\tData (t) 0.000\tBatch (t) 458.914\tLR_imgs: 0.002239 \tLR_mol: 0.002239 \tinv_tau 16.693\n",
      " 92%|█████████▎| 37/40 [2:57:45<24:29, 489.94s/it]INFO:root:Train Iters: 37 [37/40 (92%)]\tLoss: 2.778850\tData (t) 0.000\tBatch (t) 468.776\tLR_imgs: 0.002239 \tLR_mol: 0.002239 \tinv_tau 16.729\n",
      " 95%|█████████▌| 38/40 [3:05:34<16:07, 483.84s/it]INFO:root:evaluating...\n",
      "INFO:root:--------------- Zero-shot Eval ---------------\n",
      "INFO:root:Train Iters: 38 [38/40 (95%)]\tLoss: 2.777641\tTop1 Acc: 0.020000\tTop5 Acc: 0.022800\tTop10 Acc: 0.027200\t\n",
      "INFO:root:--------------- Training data eval ---------------\n",
      "INFO:root:Train Iters: 38 [38/40 (95%)]\tLoss: 2.776248\tTop1 Acc: 0.006000\tTop5 Acc: 0.010000\tTop10 Acc: 0.010400\t\n",
      "INFO:root:--------------- Eval complete ---------------\n",
      "INFO:root:Train Iters: 38 [38/40 (95%)]\tLoss: 2.775843\tData (t) 0.000\tBatch (t) 454.084\tLR_imgs: 0.001727 \tLR_mol: 0.001727 \tinv_tau 16.757\n",
      " 98%|█████████▊| 39/40 [3:16:35<08:56, 536.90s/it]INFO:root:iters 40, saving...\n",
      "INFO:root:Train Iters: 39 [39/40 (98%)]\tLoss: 2.774533\tData (t) 0.000\tBatch (t) 471.077\tLR_imgs: 0.001727 \tLR_mol: 0.001727 \tinv_tau 16.785\n",
      "100%|██████████| 40/40 [3:24:27<00:00, 306.68s/it]\n"
     ]
    }
   ],
   "source": [
    "train_model(baseline_img, baseline_mol, iters = 10, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(model_img, model_mol):\n",
    "    imgs, mols = [], []\n",
    "    for i in range(len(test_dataset)):\n",
    "        features, graphs, (labels_graphs, labels_features), _ = test_dataset[i]\n",
    "        features = torch.unsqueeze(features, 0)\n",
    "        labels_features = torch.unsqueeze(labels_features, 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model_img([graphs], features)\n",
    "            mol_features = model_mol([labels_graphs], labels_features)\n",
    "\n",
    "        imgs.append(F.normalize(image_features))\n",
    "        mols.append(F.normalize(mol_features))\n",
    "        \n",
    "    imgs = torch.cat(imgs)\n",
    "    mols = torch.cat(mols)\n",
    "\n",
    "    ##############################33\n",
    "    m_m = torch.einsum('id,jd->ij', mols, mols)\n",
    "    repeats_m = (m_m >= 0.99999).to(torch.float32)\n",
    "    m_idxs = torch.argmax(repeats_m, 1, keepdim = True)\n",
    "    m_idxs_ = torch.unique(m_idxs)\n",
    "    mol_candidates = mols[m_idxs_]\n",
    "    candidates_id = {m_idxs_[i].item(): i for i in range(len(m_idxs_))}\n",
    "\n",
    "    i_m = torch.einsum('id,jd->ij', imgs, mol_candidates)\n",
    "    ground_truth = torch.zeros(imgs.size(0), mol_candidates.size(0))\n",
    "    for i in range(len(m_idxs)): \n",
    "        ground_truth[i, candidates_id[m_idxs[i].item()]] = 1\n",
    "    topk = [1, 5, 10] ### top-x accuracy\n",
    "    topk_acc = []\n",
    "\n",
    "    for k in topk:\n",
    "        match = torch.topk(i_m, k = k, dim = -1)[1].T\n",
    "        correct_i2m = torch.sum(torch.max(ground_truth[torch.arange(imgs.size(0)), match], dim = 0)[0]).item() / imgs.size(0)\n",
    "        topk_acc.append(correct_i2m)\n",
    "\n",
    "    for i, k in enumerate(topk):\n",
    "        print(f\"Top_{k} retrieval accuracy: {topk_acc[i]}\")\n",
    "        print(f\"Random guessing accuracy: {k / np.unique(labels[test_idxs]).shape[0]}\")\n",
    "        print(f\"Folds of improvemnt: {(topk_acc[i]) * np.unique(labels[test_idxs]).shape[0]}\")\n",
    "        print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_1 retrieval accuracy: 0.034368803701255786\n",
      "Random guessing accuracy: 0.001184834123222749\n",
      "Folds of improvemnt: 29.007270323859885\n",
      "-------------------------------\n",
      "Top_5 retrieval accuracy: 0.14771976206212822\n",
      "Random guessing accuracy: 0.005924170616113744\n",
      "Folds of improvemnt: 124.67547918043623\n",
      "-------------------------------\n",
      "Top_10 retrieval accuracy: 0.29643093192333114\n",
      "Random guessing accuracy: 0.011848341232227487\n",
      "Folds of improvemnt: 250.18770654329148\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "retrieval(CSIP_img, CSIP_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_1 retrieval accuracy: 0.023132848645076007\n",
      "Random guessing accuracy: 0.001184834123222749\n",
      "Folds of improvemnt: 19.52412425644415\n",
      "-------------------------------\n",
      "Top_5 retrieval accuracy: 0.1404494382022472\n",
      "Random guessing accuracy: 0.005924170616113744\n",
      "Folds of improvemnt: 118.53932584269664\n",
      "-------------------------------\n",
      "Top_10 retrieval accuracy: 0.26305353602115006\n",
      "Random guessing accuracy: 0.011848341232227487\n",
      "Folds of improvemnt: 222.01718440185064\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "retrieval(baseline_img, baseline_mol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
